{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86bd75bf",
   "metadata": {},
   "source": [
    "# Configurações iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastf1\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from fastf1 import plotting\n",
    "from fastf1.core import Laps\n",
    "from __future__ import annotations\n",
    "from typing import List, Optional\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c06c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = './fastf1_cache'\n",
    "if not os.path.exists(CACHE_DIR):\n",
    "    os.makedirs(CACHE_DIR)\n",
    "    print(f\"Diretório de cache '{CACHE_DIR}' criado\")\n",
    "\n",
    "fastf1.Cache.clear_cache(CACHE_DIR) \n",
    "print(f\"Cache do FastF1 em '{CACHE_DIR}' limpo\")\n",
    "\n",
    "fastf1.Cache.enable_cache(CACHE_DIR)\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf106c",
   "metadata": {},
   "source": [
    "# Criação DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_main_dataset(year=2024, event='British Grand Prix', CACHE_DIR='./fastf1_cache', driver_filter=None, years_filter=None):\n",
    "    fastf1.Cache.enable_cache(CACHE_DIR)\n",
    "\n",
    "    try:\n",
    "        session = fastf1.get_session(year, event, 'R')\n",
    "        print(f\"Carregando sessão: {event} {year} - Corrida (R)\")\n",
    "        session.load(telemetry=True, weather=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar sessão: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    full_data = []\n",
    "\n",
    "    # Processar dados meteorológicos\n",
    "    weather_data = session.weather_data\n",
    "    if not weather_data.empty:\n",
    "        weather_data['Time'] = weather_data['Time'].dt.round('s')\n",
    "        weather_data_agg = weather_data.resample('1min', on='Time').mean().reset_index()\n",
    "    else:\n",
    "        weather_data_agg = pd.DataFrame()\n",
    "\n",
    "    for driver in session.drivers:\n",
    "        if driver_filter and driver != driver_filter:\n",
    "            continue\n",
    "\n",
    "        laps = session.laps.pick_driver(driver).pick_quicklaps()\n",
    "        if laps.empty:\n",
    "            continue\n",
    "\n",
    "        laps = laps.sort_values(by='LapNumber').reset_index(drop=True)\n",
    "        laps['stint_id'] = laps['Stint']\n",
    "        laps['s_lap'] = laps.groupby('stint_id').cumcount() + 1\n",
    "        laps['s_total'] = laps.groupby('stint_id')['LapNumber'].transform('count')\n",
    "        laps['s_pct'] = laps['s_lap'] / laps['s_total']\n",
    "        laps['best_s_lap'] = laps.groupby('stint_id')['LapTime'].transform('min').dt.total_seconds()\n",
    "        laps['delta_best'] = laps['LapTime'].dt.total_seconds() - laps['best_s_lap']\n",
    "        laps['delta_var'] = laps['delta_best'].diff().fillna(0)#Colocar NaN\n",
    "\n",
    "        # Estimativa de combustível\n",
    "        fuel_cons = 1.8\n",
    "        full_tank = 110\n",
    "        laps['fuel_kg'] = full_tank - (laps['LapNumber'] * fuel_cons)\n",
    "        laps['fuel_kg'] = laps['fuel_kg'].apply(lambda x: max(0, x))\n",
    "\n",
    "        laps['race_id'] = f\"{year}_{event.replace(' ', '_')}\"\n",
    "        laps['year'] = year\n",
    "        laps['race'] = event\n",
    "        laps['sc_active'] = laps['TrackStatus'].apply(lambda x: 1 if x in [2,3,4,5,6] else 0)\n",
    "\n",
    "        # Clima\n",
    "        if not weather_data_agg.empty and 'LapStartTime' in laps.columns:\n",
    "            try:\n",
    "                laps['LapStartTime_rounded'] = laps['LapStartTime'].dt.round('s')\n",
    "                laps = pd.merge_asof(\n",
    "                    laps.sort_values('LapStartTime_rounded'),\n",
    "                    weather_data_agg.sort_values('Time'),\n",
    "                    left_on='LapStartTime_rounded',\n",
    "                    right_on='Time',\n",
    "                    direction='nearest'\n",
    "                )\n",
    "                cols_to_drop = [col for col in ['LapStartTime_rounded', 'Time', 'Time_y'] if col in laps.columns]\n",
    "                laps = laps.drop(columns=cols_to_drop)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"⚠️ Erro ao mesclar clima para {driver}: {e}\")\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'race_id': laps['race_id'],\n",
    "            'year': laps['year'],\n",
    "            'race': laps['race'],\n",
    "            'drv': laps['Driver'],\n",
    "            'team': laps['Team'],\n",
    "            'lap': laps['LapNumber'],\n",
    "            's_lap': laps['s_lap'],\n",
    "            's_pct': laps['s_pct'],\n",
    "            'tyre': laps['Compound'],\n",
    "            'lap_time': laps['LapTime'].dt.total_seconds(),\n",
    "            'delta_best': laps['delta_best'],\n",
    "            'delta_var': laps['delta_var'],\n",
    "            'fuel_kg': laps['fuel_kg'],\n",
    "            'sc_active': laps['sc_active'],\n",
    "            'stint_id': laps['stint_id'],\n",
    "            'fresh_tyre': laps['FreshTyre'],\n",
    "            'speed_i1': laps['SpeedI1'],\n",
    "            'speed_i2': laps['SpeedI2'],\n",
    "            'speed_fl': laps['SpeedFL'],\n",
    "            'speed_st': laps['SpeedST'],\n",
    "            'air_temp': laps['AirTemp'] if 'AirTemp' in laps.columns else np.nan,\n",
    "            'track_temp': laps['TrackTemp'] if 'TrackTemp' in laps.columns else np.nan,\n",
    "            'humidity': laps['Humidity'] if 'Humidity' in laps.columns else np.nan,\n",
    "            'best_s_lap': laps['best_s_lap'],\n",
    "        })\n",
    "\n",
    "        full_data.append(df)\n",
    "\n",
    "    if full_data:\n",
    "        final_df = pd.concat(full_data, ignore_index=True)\n",
    "        filename = f\"tyre_wear_dataset_{year}_{event.replace(' ', '_')}.csv\"\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"Nenhum dado disponível para criar o dataset\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def generate_deltas_dataset(\n",
    "    *,\n",
    "    year: int,\n",
    "    event: str,\n",
    "    CACHE_DIR: str = CACHE_DIR,\n",
    "    driver_filter: Optional[str] = None,\n",
    "    years_filter: Optional[List[int]] = None,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    if years_filter and year not in years_filter:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    fastf1.Cache.enable_cache(CACHE_DIR)\n",
    "\n",
    "    try:\n",
    "        session = fastf1.get_session(year, event, \"R\")\n",
    "        session.load(telemetry=True)\n",
    "    except Exception as exc:\n",
    "        logger.error(\"Erro ao carregar telemetria %s %d: %s\", event, year, exc)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_delta: List[pd.DataFrame] = []\n",
    "\n",
    "    for drv_num in session.drivers:\n",
    "        drv_code = session.get_driver(drv_num)[\"Abbreviation\"]\n",
    "        if driver_filter and drv_code != driver_filter:\n",
    "            continue\n",
    "\n",
    "        laps = session.laps.pick_driver(drv_code).pick_quicklaps()\n",
    "        if laps.empty:\n",
    "            continue\n",
    "\n",
    "        base_df = pd.DataFrame(\n",
    "            {\n",
    "                \"race_id\": f\"{year}_{event.replace(' ', '_')}\",\n",
    "                \"drv\": drv_code,\n",
    "                \"lap\": laps[\"LapNumber\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            tel = laps.get_telemetry()\n",
    "            if tel.empty:\n",
    "                all_delta.append(base_df)\n",
    "                continue\n",
    "\n",
    "            tel.add_driver_ahead()\n",
    "            laps_time = laps[[\"LapNumber\", \"LapStartTime\", \"Time\"]].rename(\n",
    "                columns={\"Time\": \"LapEndTime\"}\n",
    "            )\n",
    "            tel = tel.sort_values(\"SessionTime\").reset_index(drop=True)\n",
    "            laps_time = laps_time.sort_values(\"LapStartTime\").reset_index(drop=True)\n",
    "\n",
    "            tel = pd.merge_asof(\n",
    "                tel,\n",
    "                laps_time,\n",
    "                left_on=\"SessionTime\",\n",
    "                right_on=\"LapStartTime\",\n",
    "                direction=\"backward\",\n",
    "            )\n",
    "            tel = tel[\n",
    "                (tel[\"SessionTime\"] >= tel[\"LapStartTime\"]) &\n",
    "                (tel[\"SessionTime\"] <= tel[\"LapEndTime\"])\n",
    "            ]\n",
    "            if tel.empty:\n",
    "                all_delta.append(base_df)\n",
    "                continue\n",
    "\n",
    "            dist = (\n",
    "                tel.groupby(\"LapNumber\")[\"DistanceToDriverAhead\"].mean().reset_index()\n",
    "            ).rename(columns={\"LapNumber\": \"lap\", \"DistanceToDriverAhead\": \"delta_s\"})\n",
    "\n",
    "            base_df = base_df.merge(dist, on=\"lap\", how=\"left\")\n",
    "        except Exception as exc:\n",
    "            logger.warning(\"Erro ao processar telemetria %s: %s\", drv_code, exc)\n",
    "            base_df[\"delta_s\"] = np.nan\n",
    "\n",
    "        all_delta.append(base_df)\n",
    "\n",
    "    return pd.concat(all_delta, ignore_index=True) if all_delta else pd.DataFrame()\n",
    "\n",
    "def merge_fastf1_dataframes(tyre_df: pd.DataFrame, delta_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if tyre_df.empty:\n",
    "        logger.warning(\"DataFrame de pneus vazio.\")\n",
    "        return pd.DataFrame()\n",
    "    if delta_df.empty:\n",
    "        logger.warning(\"DataFrame de delta vazio – retornando apenas pneus.\")\n",
    "        return tyre_df\n",
    "    return tyre_df.merge(delta_df, on=[\"race_id\", \"drv\", \"lap\"], how=\"left\")\n",
    "\n",
    "\n",
    "def add_analysis_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    FUEL_IMPACT = 0.03  # s/kg\n",
    "    df = df.copy()\n",
    "    df[\"delta_adjusted_fuel\"] = df[\"delta_best\"] - df[\"fuel_kg\"] * FUEL_IMPACT\n",
    "\n",
    "    avg_circuit_tyre = (\n",
    "        df.groupby([\"race_id\", \"tyre\"])[\"delta_best\"].mean().reset_index()\n",
    "    ).rename(columns={\"delta_best\": \"avg_delta_best_circuit_tyre\"})\n",
    "    df = df.merge(avg_circuit_tyre, on=[\"race_id\", \"tyre\"], how=\"left\")\n",
    "\n",
    "    avg_driver_tyre = (\n",
    "        df.groupby([\"drv\", \"tyre\"])[\"delta_best\"].mean().reset_index()\n",
    "    ).rename(columns={\"delta_best\": \"avg_delta_best_driver_tyre\"})\n",
    "    df = df.merge(avg_driver_tyre, on=[\"drv\", \"tyre\"], how=\"left\")\n",
    "\n",
    "    if \"best_s_lap\" in df.columns:\n",
    "        df[\"is_stint_fastest_lap\"] = (\n",
    "            df[\"lap_time\"] == df[\"best_s_lap\"]\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        df[\"is_stint_fastest_lap\"] = np.nan\n",
    "\n",
    "    def _track(row):\n",
    "        if pd.isna(row[\"track_temp\"]):\n",
    "            return \"WET_OR_INTERMEDIATE\" if row[\"tyre\"] in [\"INTERMEDIATE\", \"WET\"] else \"UNKNOWN_TEMP\"\n",
    "        if row[\"tyre\"] in [\"INTERMEDIATE\", \"WET\"]:\n",
    "            return \"WET_OR_INTERMEDIATE\"\n",
    "        if row[\"track_temp\"] > 25:\n",
    "            return \"DRY_HOT\"\n",
    "        if row[\"track_temp\"] < 15:\n",
    "            return \"DRY_COLD\"\n",
    "        return \"DRY_NORMAL\"\n",
    "\n",
    "    df[\"track_condition\"] = df.apply(_track, axis=1)\n",
    "    logger.info(\"Features adicionadas. Shape: %s\", df.shape)\n",
    "    return df\n",
    "\n",
    "def load_multi_year_data(\n",
    "    *,\n",
    "    years: List[int],\n",
    "    events: List[str],\n",
    "    driver_filter: Optional[str] = None,\n",
    "    CACHE_DIR: str = CACHE_DIR,\n",
    ") -> pd.DataFrame:\n",
    "    all_main, all_delta = [], []\n",
    "    for yr in years:\n",
    "        for ev in events:\n",
    "            main_df = generate_main_dataset(\n",
    "                year=yr,\n",
    "                event=ev,\n",
    "                CACHE_DIR=CACHE_DIR,\n",
    "                driver_filter=driver_filter,\n",
    "                years_filter=years,\n",
    "            )\n",
    "            delta_df = generate_deltas_dataset(\n",
    "                year=yr,\n",
    "                event=ev,\n",
    "                CACHE_DIR=CACHE_DIR,\n",
    "                driver_filter=driver_filter,\n",
    "                years_filter=years,\n",
    "            )\n",
    "            if not main_df.empty:\n",
    "                all_main.append(main_df)\n",
    "            if not delta_df.empty:\n",
    "                all_delta.append(delta_df)\n",
    "\n",
    "    main_all = pd.concat(all_main, ignore_index=True) if all_main else pd.DataFrame()\n",
    "    delta_all = pd.concat(all_delta, ignore_index=True) if all_delta else pd.DataFrame()\n",
    "\n",
    "    combined = merge_fastf1_dataframes(main_all, delta_all)\n",
    "    if combined.empty:\n",
    "        logger.error(\"Nenhum dado disponível após merge\")\n",
    "        return combined\n",
    "\n",
    "    return add_analysis_features(combined)\n",
    "    \n",
    "def fill_outliers_with_median(df, col, n_std=2.5, group_by_cols=['drv', 'tyre']):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Calcular estatísticas por grupo (incluindo mediana para preenchimento de NA e outliers)\n",
    "    valid_group_by_cols = [g_col for g_col in group_by_cols if g_col in df_copy.columns]\n",
    "    \n",
    "    # Calcular a mediana global da coluna como fallback\n",
    "    global_median_col = df_copy[col].median()\n",
    "\n",
    "    if not valid_group_by_cols:\n",
    "        print(f\"Atenção: Nenhuma das colunas de agrupamento {group_by_cols} encontrada para '{col}'. Processando sem agrupamento.\")\n",
    "        df_copy['group_mean'] = df_copy[col].mean()\n",
    "        df_copy['group_std'] = df_copy[col].std()\n",
    "        df_copy['group_median'] = df_copy[col].median()\n",
    "    else:\n",
    "        # Calcular estatísticas agrupadas\n",
    "        df_copy['group_mean'] = df_copy.groupby(valid_group_by_cols)[col].transform('mean')\n",
    "        df_copy['group_std'] = df_copy.groupby(valid_group_by_cols)[col].transform('std')\n",
    "        df_copy['group_median'] = df_copy.groupby(valid_group_by_cols)[col].transform('median')\n",
    "\n",
    "    # --- 1. Tratar valores ausentes DENTRO da função de outliers ---\n",
    "    na_count_before = df_copy[col].isnull().sum()\n",
    "    if na_count_before > 0:\n",
    "        # Preencher NaNs com a mediana do grupo; se a mediana do grupo for NaN, usa a mediana global da coluna\n",
    "        df_copy[col] = df_copy[col].fillna(df_copy['group_median']).fillna(global_median_col)\n",
    "        print(f\"  Preenchidos {na_count_before} NA's em '{col}' (com mediana do grupo ou global).\")\n",
    "\n",
    "    # Recalcular estatísticas após preencher os NaNs para garantir que a detecção de outliers use dados completos\n",
    "    if not valid_group_by_cols:\n",
    "        df_copy['group_mean'] = df_copy[col].mean()\n",
    "        df_copy['group_std'] = df_copy[col].std()\n",
    "        df_copy['group_median'] = df_copy[col].median()\n",
    "    else:\n",
    "        df_copy['group_mean'] = df_copy.groupby(valid_group_by_cols)[col].transform('mean')\n",
    "        df_copy['group_std'] = df_copy.groupby(valid_group_by_cols)[col].transform('std')\n",
    "        df_copy['group_median'] = df_copy.groupby(valid_group_by_cols)[col].transform('median')\n",
    "\n",
    "\n",
    "    # --- 2. Identificar e preencher outliers ---\n",
    "    upper_outlier_condition = (df_copy[col] > df_copy['group_mean'] + n_std * df_copy['group_std'])\n",
    "    lower_outlier_condition = (df_copy[col] < df_copy['group_mean'] - n_std * df_copy['group_std'])\n",
    "    outlier_condition = upper_outlier_condition | lower_outlier_condition\n",
    "\n",
    "    num_outliers_before = outlier_condition.sum()\n",
    "    print(f\"Número de outliers identificados em '{col}' (com n_std={n_std}): {num_outliers_before}\")\n",
    "\n",
    "    # Preencher outliers com a mediana do grupo (ou global se a do grupo for NaN)\n",
    "    df_copy.loc[outlier_condition, col] = df_copy.loc[outlier_condition, 'group_median'].fillna(global_median_col)\n",
    "\n",
    "    # Remover colunas auxiliares\n",
    "    df_filled = df_copy.drop(columns=['group_mean', 'group_std', 'group_median'])\n",
    "\n",
    "    return df_filled\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    YEARS = [2025]  \n",
    "    EVENTS = [\"Spanish Grand Prix\"]  \n",
    "    DRIVER = None  \n",
    "\n",
    "    df_final = load_multi_year_data(\n",
    "        years=YEARS,\n",
    "        events=EVENTS,\n",
    "        driver_filter=DRIVER,\n",
    "    )\n",
    "    \n",
    "    df_2025 = fill_outliers_with_median(df_final, 'lap_time', n_std=2.5)\n",
    "\n",
    "    if not df_final.empty:\n",
    "        driver_tag = DRIVER or \"ALL\"\n",
    "        fname = f\"analyzed_fastf1_data_{driver_tag}_{'_'.join(map(str, YEARS))}\"\n",
    "        logger.info(\"Dataframe salvo como '%s' (%d linhas).\", fname, len(df_final))\n",
    "    else:\n",
    "        logger.warning(\"Pipeline terminou sem dados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1818ed6",
   "metadata": {},
   "source": [
    "# Integração PostGreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a3aab",
   "metadata": {},
   "source": [
    "**Enviando o DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SALVANDO OS\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def save_data_to_sql(df, table_name, db_type, db_details, if_exists='replace', index=False):\n",
    "    conn_str = \"\"\n",
    "    try:\n",
    "        if db_type == 'sqlite':\n",
    "            conn_str = f\"sqlite:///{db_details['database']}\"\n",
    "        elif db_type == 'mysql':\n",
    "            conn_str = (\n",
    "                f\"mysql+mysqlconnector://{db_details['user']}:{db_details['password']}\"\n",
    "                f\"@{db_details['host']}/{db_details['database']}\"\n",
    "            )\n",
    "        elif db_type == 'postgresql':\n",
    "            conn_str = (\n",
    "                f\"postgresql+psycopg2://{db_details['user']}:{db_details['password']}\"\n",
    "                f\"@{db_details['host']}:{db_details.get('port', 5432)}/{db_details['database']}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Erro: Tipo de banco de dados '{db_type}' não suportado\")\n",
    "            return False\n",
    "\n",
    "        engine = create_engine(conn_str)\n",
    "        df.to_sql(name=table_name, con=engine, if_exists=if_exists, index=index)\n",
    "        print(f\"Dados salvos com sucesso na tabela '{table_name}' no banco de dados {db_type}\")\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"Erro: O driver para o banco de dados '{db_type}' não está instalado. Detalhes: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar dados no banco de dados {db_type}: {e}\")\n",
    "        return False\n",
    "\n",
    "postgresql_db_details = {\n",
    "    'user': os.environ.get('PG_USER'),        \n",
    "    'password': os.environ.get('PG_PASSWORD'),\n",
    "    'host': os.environ.get('PG_HOST'),\n",
    "    'port': int(os.environ.get('PG_PORT')),\n",
    "    'database': os.environ.get('PG_DATABASE')\n",
    "}\n",
    "\n",
    "table_name = 'fastf1_analysis_data'\n",
    "\n",
    "print(f\"1. Carregando dados do DF:\")\n",
    "df_fastf1 = df_final.copy() if df_final is not None else pd.DataFrame()\n",
    "df_fastf1.to_csv('fastf1_analysis_data.csv', index=False)\n",
    "if df_fastf1 is not None and not df_fastf1.empty:\n",
    "    print(f\"\\n2. Tentando salvar o DataFrame na tabela '{table_name}' no PostgreSQL...\")\n",
    "    success = save_data_to_sql(df_fastf1, table_name, 'postgresql', postgresql_db_details, if_exists='replace')\n",
    "\n",
    "    if success:\n",
    "        print(\"\\nDados salvos com sucesso no PostgreSQL.\")\n",
    "    else:\n",
    "        print(\"\\nNão foi possível salvar os dados no PostgreSQL.\")\n",
    "else:\n",
    "    print(\"\\nO DataFrame não pôde ser carregado ou está vazio. Não é possível prosseguir para salvar no PostgreSQL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8862ab",
   "metadata": {},
   "source": [
    "**Carregando uma tabela em DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea265198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_postgresql(table_name):\n",
    "    try:\n",
    "        conn_str = f\"postgresql+psycopg2://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DATABASE}\"\n",
    "        engine = create_engine(conn_str)\n",
    "        print(\"Conexão com o PostgreSQL configurada com sucesso\")\n",
    "\n",
    "        # Carregando os dados em um DataFrame\n",
    "        print(f\"Carregando dados da tabela '{table_name}' para um DataFrame\")\n",
    "        sql_query = f\"SELECT * FROM {table_name}\"\n",
    "        df_from_pg = pd.read_sql_query(sql_query, con=engine)\n",
    "\n",
    "        print(\"\\nDados carregados com sucesso do PostgreSQL para o DataFrame\")\n",
    "        print(f\"\\nDataframe importado com total de {len(df_from_pg)} linhas\")\n",
    "        return df_from_pg\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao conectar ou carregar dados do PostgreSQL: {e}\")\n",
    "\n",
    "load_dotenv()\n",
    "PG_USER = os.environ.get('PG_USER')\n",
    "PG_PASSWORD = os.environ.get('PG_PASSWORD')\n",
    "PG_HOST = os.environ.get('PG_HOST')\n",
    "PG_PORT = int(os.environ.get('PG_PORT', 5432)) \n",
    "PG_DATABASE = os.environ.get('PG_DATABASE')\n",
    "\n",
    "table_name = 'fastf1_analysis_data'\n",
    "\n",
    "df = load_data_from_postgresql(table_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba2044",
   "metadata": {},
   "source": [
    "# Análise dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aac030",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_TO_ANALYZE = 'HAM'\n",
    "\n",
    "if df_final is not None and not df_final.empty:\n",
    "    try:\n",
    "        df_visual = load_data_from_postgresql(table_name)\n",
    "        driver_df_visual = df_visual[df_visual['drv'] == DRIVER_TO_ANALYZE].copy()\n",
    "\n",
    "        if driver_df_visual.empty:\n",
    "            print(f\"Nenhum dado encontrado para o piloto '{DRIVER_TO_ANALYZE}' para visualização\")\n",
    "            print(f\"Pilotos disponíveis no dataset: {df_visual['drv'].unique().tolist()}\")\n",
    "        else:\n",
    "            print(f\"Dados filtrados para visualização do piloto: {DRIVER_TO_ANALYZE}. Total de {len(driver_df_visual)} voltas\")\n",
    "            driver_df_visual['stint_group_id'] = (driver_df_visual['s_lap'] == 1).cumsum()\n",
    "\n",
    "            compound_colors = {\n",
    "                'SOFT': '#FF3333',\n",
    "                'MEDIUM': '#FFCC00',\n",
    "                'HARD': '#CCCCCC',\n",
    "                'INTERMEDIATE': '#009900',\n",
    "                'WET': '#0000FF',\n",
    "                'UNKNOWN': '#800080',\n",
    "                'TEST_UNKNOWN': '#808000'\n",
    "            }\n",
    "\n",
    "            for year in sorted(driver_df_visual['year'].unique()):\n",
    "                df_year = driver_df_visual[driver_df_visual['year'] == year]\n",
    "\n",
    "                print(f\"\\n Ano: {year} | Voltas: {len(df_year)}\")\n",
    "\n",
    "                # 1. Tempo de volta por stint\n",
    "                plt.figure(figsize=(12, 7))\n",
    "                for (stint_group_id, tyre), group in df_year.groupby(['stint_group_id', 'tyre']):\n",
    "                    color = compound_colors.get(tyre, '#000000')\n",
    "                    plt.plot(group['s_lap'], group['lap_time'], marker='o', linestyle='-', color=color,\n",
    "                             label=f'Stint {stint_group_id} ({tyre})')\n",
    "                plt.title(f'Tempo de Volta por Stint - {DRIVER_TO_ANALYZE} - {year}')\n",
    "                plt.xlabel('Volta do Stint')\n",
    "                plt.ylabel('Tempo de Volta (s)')\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.legend(title='Stint/Pneu', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                # 2. Degradação (delta_best)\n",
    "                plt.figure(figsize=(12, 7))\n",
    "                for (stint_group_id, tyre), group in df_year.groupby(['stint_group_id', 'tyre']):\n",
    "                    color = compound_colors.get(tyre, '#000000')\n",
    "                    plt.plot(group['s_lap'], group['delta_best'], marker='o', linestyle='-', color=color,\n",
    "                             label=f'Stint {stint_group_id} ({tyre})')\n",
    "                plt.title(f'Degradação do Pneu (Δ para melhor volta) - {DRIVER_TO_ANALYZE} - {year}')\n",
    "                plt.xlabel('Volta do Stint')\n",
    "                plt.ylabel('Delta para Melhor Volta (s)')\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.legend(title='Stint/Pneu', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                # 3. Tempo médio por stint\n",
    "                plt.figure(figsize=(12, 7))\n",
    "                stint_avg = df_year.groupby(['stint_group_id', 'tyre'])['lap_time'].mean().reset_index()\n",
    "                stint_avg['label'] = stint_avg.apply(lambda row: f'Stint {row.stint_group_id} ({row.tyre})', axis=1)\n",
    "                plt.bar(stint_avg['label'], stint_avg['lap_time'],\n",
    "                        color=[compound_colors.get(c, '#000000') for c in stint_avg['tyre']])\n",
    "                plt.title(f'Tempo Médio por Stint - {DRIVER_TO_ANALYZE} - {year}')\n",
    "                plt.xlabel('Stint')\n",
    "                plt.ylabel('Tempo Médio de Volta (s)')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                # 4. Volta vs combustível\n",
    "                plt.figure(figsize=(12, 7))\n",
    "                for tyre, group in df_year.groupby('tyre'):\n",
    "                    color = compound_colors.get(tyre, '#000000')\n",
    "                    plt.scatter(group['fuel_kg'], group['lap_time'], alpha=0.6, label=tyre, color=color)\n",
    "                plt.title(f'Tempo de Volta vs Combustível - {DRIVER_TO_ANALYZE} - {year}')\n",
    "                plt.xlabel('Peso Estimado de Combustível (kg)')\n",
    "                plt.ylabel('Tempo de Volta (s)')\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                # 5. Volta vs % do stint\n",
    "                plt.figure(figsize=(12, 7))\n",
    "                for tyre, group in df_year.groupby('tyre'):\n",
    "                    color = compound_colors.get(tyre, '#000000')\n",
    "                    plt.scatter(group['s_pct'] * 100, group['lap_time'], alpha=0.6, label=tyre, color=color)\n",
    "                plt.title(f'Tempo de Volta vs % do Stint - {DRIVER_TO_ANALYZE} - {year}')\n",
    "                plt.xlabel('% do Stint')\n",
    "                plt.ylabel('Tempo de Volta (s)')\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            print(\"\\nVisualização de análise de desgaste por ano concluída.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao carregar ou visualizar o arquivo CSV: {e}\")\n",
    "else:\n",
    "    logger.error(f\"Não foi possível carregar o DataFrame para visualização. Verifique se o DataFrame está vazio ou se ocorreu um erro anterior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed14fe",
   "metadata": {},
   "source": [
    "# Algoritmo de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbfa340",
   "metadata": {},
   "source": [
    "**Tratamento de dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    if df is None or df.empty:\n",
    "        print(\"Nenhum dado disponível para processamento.\")\n",
    "        return df\n",
    "    else:\n",
    "        # Preencher outliers com a mediana para colunas específicas\n",
    "        cols_to_process = ['lap_time', 'delta_best', 'delta_var', 'fuel_kg',\n",
    "                        'speed_i1', 'speed_i2', 'speed_fl', 'speed_st', 'delta_s']\n",
    "        for col in cols_to_process:\n",
    "            if col in df.columns:\n",
    "                df = fill_outliers_with_median(df, col, n_std=2.5, group_by_cols=['drv', 'tyre'])\n",
    "\n",
    "        #Converter colunas categóricas para numéricas\n",
    "        df['fresh_tyre'] = df['fresh_tyre'].astype(int)\n",
    "        cols_to_drop = ['race']\n",
    "        df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True)\n",
    "        categorical_cols = ['tyre', 'track_condition']\n",
    "        df_preprocessed = pd.get_dummies(df, columns=[col for col in categorical_cols if col in df.columns], drop_first=False)\n",
    "        for col in df_preprocessed.select_dtypes(include='bool').columns:\n",
    "            df_preprocessed[col] = df_preprocessed[col].astype(int)\n",
    "        \n",
    "        #Escalando features numéricas\n",
    "        numerical_cols_to_scale = df_preprocessed.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "        if 'year' in numerical_cols_to_scale:\n",
    "            numerical_cols_to_scale.remove('year')\n",
    "        encoded_cols_prefix = [col + '_' for col in categorical_cols]\n",
    "        numerical_cols_to_scale = [\n",
    "            col for col in numerical_cols_to_scale\n",
    "            if not any(col.startswith(p) for p in encoded_cols_prefix) and df_preprocessed[col].dtype != 'uint8'\n",
    "        ]\n",
    "        scaler = MinMaxScaler()\n",
    "        df_preprocessed[numerical_cols_to_scale] = scaler.fit_transform(df_preprocessed[numerical_cols_to_scale])\n",
    "    return df_preprocessed\n",
    "\n",
    "df = load_data_from_postgresql(table_name)\n",
    "df = process_data(df_2025)\n",
    "save_data_to_sql(df, table_name, 'postgresql', postgresql_db_details, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5e78f",
   "metadata": {},
   "source": [
    "**Arquitetura do Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50713cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, n_timesteps):\n",
    "    X_sequences, y_sequences = [], []\n",
    "    for i in range(len(X) - n_timesteps):\n",
    "        X_sequences.append(X.iloc[i:(i + n_timesteps)].values)\n",
    "        y_sequences.append(y.iloc[i + n_timesteps])\n",
    "    return np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "def build_lstm_model(n_timesteps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=100, activation='relu', input_shape=(n_timesteps, n_features), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=100, activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "DRIVER_TO_ANALYZE = 'VER'\n",
    "YEAR_TO_ANALYZE = 2024, 2023, 2022\n",
    "df_driver = df[df['drv'] == DRIVER_TO_ANALYZE].copy() \n",
    "df_driver = df_driver.reset_index(drop=True)\n",
    "df_driver = df[\n",
    "        (df['drv'] == DRIVER_TO_ANALYZE) \n",
    "    ].copy()\n",
    "df_driver = df_driver.reset_index(drop=True)\n",
    "drop = ['drv', 'year', 'race_id', 'team'] \n",
    "df_driver.drop(columns=[col for col in drop if col in df_driver.columns], inplace=True)\n",
    "\n",
    "feat = [\n",
    "    'lap', \n",
    "    's_lap', \n",
    "    's_pct', \n",
    "    'fuel_kg', \n",
    "    'sc_active', \n",
    "    'stint_id', \n",
    "    'fresh_tyre', \n",
    "    'speed_i1', 'speed_i2', 'speed_fl', 'speed_st', \n",
    "    'air_temp', 'track_temp', 'humidity', \n",
    "    'delta_best', \n",
    "    'delta_var', \n",
    "    'avg_delta_best_circuit_tyre', \n",
    "    'avg_delta_best_driver_tyre', \n",
    "    'is_stint_fastest_lap', \n",
    "    'tyre_HARD', 'tyre_MEDIUM', 'tyre_SOFT',\n",
    "    'track_condition_DRY_HOT',  \n",
    "]\n",
    "\n",
    "target = 'lap_time'\n",
    "\n",
    "X_df = df_driver[feat]\n",
    "y_df = df_driver[target]\n",
    "\n",
    "N_FEATURES = X_df.shape[1]\n",
    "N_TIMESTEPS = 5\n",
    "\n",
    "X_sequences, y_sequences = create_sequences(X_df, y_df, N_TIMESTEPS)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)\n",
    "\n",
    "model = build_lstm_model(n_timesteps=N_TIMESTEPS, n_features=N_FEATURES)\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "print(\"\\nAvaliando o modelo no conjunto de validação:\")\n",
    "val_loss, val_mae, val_mse = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Perda de Validação (MSE): {val_loss:.4f}\")\n",
    "print(f\"Erro Médio Absoluto de Validação (MAE): {val_mae:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot da Perda (Loss)\n",
    "plt.subplot(1, 2, 1) # 1 linha, 2 colunas, 1º gráfico\n",
    "plt.plot(history.history['loss'], label='Perda de Treinamento (MSE)')\n",
    "plt.plot(history.history['val_loss'], label='Perda de Validação (MSE)')\n",
    "plt.title('Perda do Modelo ao Longo das Épocas')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot do MAE\n",
    "plt.subplot(1, 2, 2) # 1 linha, 2 colunas, 2º gráfico\n",
    "plt.plot(history.history['mae'], label='MAE de Treinamento')\n",
    "plt.plot(history.history['val_mae'], label='MAE de Validação')\n",
    "plt.title('MAE do Modelo ao Longo das Épocas')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout() # Ajusta o layout para evitar sobreposição\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de Valores Reais vs. Valores Preditos no conjunto de validação\n",
    "print(\"\\nGerando previsões no conjunto de validação...\")\n",
    "y_pred = model.predict(X_val).flatten() # Fazer previsões e achatar para 1D\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, y_pred, alpha=0.6) # Scatter plot para comparar\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2) # Linha ideal (y=x)\n",
    "plt.title(f'Valores Reais vs. Preditos (Piloto: {DRIVER_TO_ANALYZE}, Ano: {YEAR_TO_ANALYZE})')\n",
    "plt.xlabel(f'Tempo de Volta Real (Escalado)')\n",
    "plt.ylabel(f'Tempo de Volta Predito (Escalado)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb6025",
   "metadata": {},
   "source": [
    "**Testando o modelo na corrida de 2025**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db693e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025_driver = df[df['drv'] == DRIVER_TO_ANALYZE].copy()\n",
    "df_2025_driver = df_2025_driver.reset_index(drop=True)\n",
    "\n",
    "if df_2025_driver.empty:\n",
    "    print(f\"Aviso: Nenhum dado de 2025 encontrado para o piloto '{DRIVER_TO_ANALYZE}'. Não é possível testar.\")\n",
    "    exit()\n",
    "\n",
    "df_2025_driver.drop(columns=[col for col in drop if col in df_2025_driver], inplace=True, errors='ignore')\n",
    "\n",
    "X_test_template = pd.DataFrame(columns=feat)\n",
    "\n",
    "X_df_test = df_2025_driver[feat]\n",
    "y_df_test = df_2025_driver[target] \n",
    "\n",
    "if len(X_df_test) < N_TIMESTEPS + 1:\n",
    "    print(f\"Aviso: Dados de teste insuficientes para criar sequências (necessário pelo menos {N_TIMESTEPS + 1} voltas).\")\n",
    "    X_sequences_test = np.array([])\n",
    "    y_sequences_test = np.array([])\n",
    "else:\n",
    "    X_sequences_test, y_sequences_test = create_sequences(X_df_test, y_df_test, N_TIMESTEPS)\n",
    "    print(f\"Formato das sequências de entrada de teste (X_sequences_test): {X_sequences_test.shape}\")\n",
    "    print(f\"Formato das sequências de saída de teste (y_sequences_test): {y_sequences_test.shape}\")\n",
    "\n",
    "\n",
    "if X_sequences_test.shape[0] > 0:\n",
    "    print(\"\\nFazendo previsões nos dados de teste de 2025...\")\n",
    "    y_pred_test = model.predict(X_sequences_test).flatten()\n",
    "\n",
    "    test_loss, test_mae, test_mse = model.evaluate(X_sequences_test, y_sequences_test, verbose=0)\n",
    "    print(f\"Perda de Teste (MSE) em 2025: {test_loss:.4f}\")\n",
    "    print(f\"Erro Médio Absoluto de Teste (MAE) em 2025: {test_mae:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_sequences_test, y_pred_test, alpha=0.6)\n",
    "    plt.plot([y_sequences_test.min(), y_sequences_test.max()], [y_sequences_test.min(), y_sequences_test.max()], 'r--', lw=2)\n",
    "    plt.title(f'Valores Reais vs. Preditos (Teste 2025 - Piloto: {DRIVER_TO_ANALYZE})')\n",
    "    plt.xlabel(f'Tempo de Volta Real (Escalado)')\n",
    "    plt.ylabel(f'Tempo de Volta Predito (Escalado)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Não foi possível gerar previsões ou avaliar o modelo para 2025 devido à falta de dados de teste suficientes.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1pit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
